# -*- coding: utf-8 -*-
"""TP_3_Lenardon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BDNryq-EET8zmK71VWgvIQHuehfTgMfh
"""

### importo librerias
import pandas as pd
import numpy as np
from tensorflow import keras
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve, roc_auc_score,accuracy_score,auc

"""##CARGA DE DATOS

Problema: La organizacion quiere obtener en base a datos un modelo que optimice la respuesta a las campañas de marketing que realizan, el objetivo es predecir que cliente responde a la campaña de marketing, para mejorar la respuesta y minimizar costos de campaña. El dataset cuenta con 32 variables y 2240 datos. Dentro de las variables tenemos cuantitativas discretas y continuas, y variables cualitativas nominales y ordinales.
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#https://drive.google.com/file/d/1N3SXtQPoJUZbfyfiPCmMUyjlWrS-72RA/view?usp=sharing
fileDownloaded = drive.CreateFile({'id':'1N3SXtQPoJUZbfyfiPCmMUyjlWrS-72RA'})

# Se descarga el archivo.
fileDownloaded.GetContentFile('marketing_campaign.csv')

#https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign
##Importo dataset
marketing=pd.read_csv("/content/marketing_campaign.csv", sep=';')
marketing.head(10)

##veo dimension
marketing.shape

## analizo tipos de datos que tiene el dataset
marketing.dtypes

### veo nombre de columnas
marketing.columns

"""##PREPARACION DE DATOS

Analizo outliers
"""

###Analizo Year_Birth
plt.boxplot(marketing['Year_Birth'])
plt.title('Year_Birth')
plt.ylabel('Year_Birth')

unique_year_birth=marketing['Year_Birth'].unique()
unique_year_birth= sorted(unique_year_birth)
print(unique_year_birth)

### reemplazo atipicos con faltantes
marketing['Year_Birth']=marketing['Year_Birth'].replace({1893: None, 1899: None, 1900: None}, inplace=False)

###Analizo Income
marketing['Income']=marketing['Income'].fillna(0)

marketing['Income']=marketing['Income'].astype('int64')

plt.boxplot(marketing['Income'])
plt.title('Income')
plt.ylabel('Income')

plt.hist(marketing['Income'], bins=500, color='blue', alpha=0.5)

income_120000=marketing['Income']>120000
income_120000=marketing[income_120000]

income_120000['Income'].unique()

## reemplazo atipicos con faltantes
for i in [0,157243, 162397, 153924, 160803, 157733, 157146, 156924, 666666]:
  marketing['Income']=marketing['Income'].replace({i: None}, inplace=False)

marketing.head(10)

marketing['Kidhome'].unique()

marketing['Teenhome'].unique()

plt.boxplot(marketing['Recency'])

plt.boxplot(marketing['MntWines'])

plt.boxplot(marketing['MntFruits'])

## Analizo MntMeatProducts
plt.boxplot(marketing['MntMeatProducts'])

unique_MntMeatProducts=marketing['MntMeatProducts']>1200
unique_MntMeatProducts=marketing[unique_MntMeatProducts]
unique_MntMeatProducts['MntMeatProducts'].unique()

### reemplazo atipicos de MntMeatProducts
for i in [1725, 1582, 1622, 1607]:
  marketing['MntMeatProducts']=marketing['MntMeatProducts'].replace({i: None}, inplace=False)

plt.boxplot(marketing['MntFishProducts'])

plt.boxplot(marketing['MntSweetProducts'])

unique_MntSweetProducts=marketing['MntSweetProducts']>250
unique_MntSweetProducts=marketing[unique_MntSweetProducts]
unique_MntSweetProducts['MntSweetProducts'].unique()

###Reemplazo atipicos MntSweetProducts
marketing['MntSweetProducts']=marketing['MntSweetProducts'].replace({263: None, 262: None}, inplace=False)

plt.boxplot(marketing['MntGoldProds'])

plt.boxplot(marketing['NumDealsPurchases'])

unique_NumDealsPurchases=marketing['NumDealsPurchases']>12
unique_NumDealsPurchases=marketing[unique_NumDealsPurchases]
unique_NumDealsPurchases['NumDealsPurchases'].unique()

### reemplazo atipicos con faltantes
marketing['NumDealsPurchases']=marketing['NumDealsPurchases'].replace({15:None,13:None},inplace=False)

plt.boxplot(marketing['NumWebPurchases'])

unique_NumWebPurchases=marketing['NumWebPurchases']>20
unique_NumWebPurchases=marketing[unique_NumWebPurchases]
unique_NumWebPurchases['NumWebPurchases'].unique()

### reemplazo atipicos con faltantes
marketing['NumWebPurchases']=marketing['NumWebPurchases'].replace({27:None,25:None,23:None}, inplace=False)

plt.boxplot(marketing['NumCatalogPurchases'])

unique_NumCatalogPurchases=marketing['NumCatalogPurchases']>20
unique_NumCatalogPurchases=marketing[unique_NumCatalogPurchases]
unique_NumCatalogPurchases['NumCatalogPurchases'].unique()

### reemplazo atipicos con faltantes
marketing['NumCatalogPurchases']=marketing['NumCatalogPurchases'].replace({28:None,22:None}, inplace=False)

plt.boxplot(marketing['NumStorePurchases'])

plt.boxplot(marketing['NumWebVisitsMonth'])

"""Elimino variables constantes"""

marketing['Z_CostContact'].unique()

marketing['Z_Revenue'].unique()

marketing=marketing.drop(['Z_CostContact','Z_Revenue'], axis=1)

###Elimino ID
marketing=marketing.drop(['ID'], axis=1)

marketing.head()

### Veo dimension del dataset
marketing.shape

"""Conversion de datos"""

marketing['Education'].unique()

### Numerizo la variable EDUCATION
marketing['Education']=marketing['Education'].map({'Basic':1, '2n Cycle':2,'Graduation':3,'Master':4,'PhD':5 })
marketing['Education'].head(10)

marketing['Marital_Status'].unique()

### transformo a dummies los valores de Marital_Status
marketing=pd.get_dummies(marketing, columns=['Marital_Status'])

### transformo los valores de Dt_Customer a datetime
marketing['Dt_Customer']=pd.to_datetime(marketing['Dt_Customer'],format='%Y-%m-%d')

### Analizo cantidad de valores faltantes
marketing.isna().sum()

### transformo la fecha de cliente a el año de ingreso como cliente
marketing['Dt_Customer']=marketing['Dt_Customer'].dt.year

marketing.head(10)

### Normalizo datos
marketing_norm= ((marketing-marketing.min())/(marketing.max()-marketing.min()))

marketing_norm.head(10)

marketing_norm.columns

"""Imputacion de datos faltantes"""

### imputo por vecino mas cercano
imp=KNNImputer(n_neighbors=3, weights='distance')
temp=imp.fit_transform(marketing_norm.to_numpy())

print(temp)

marketing_norm=pd.DataFrame(temp, columns=['Year_Birth', 'Education', 'Income', 'Kidhome', 'Teenhome',
       'Dt_Customer', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',
       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',
       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',
       'Complain', 'Response', 'Marital_Status_Absurd', 'Marital_Status_Alone',
       'Marital_Status_Divorced', 'Marital_Status_Married',
       'Marital_Status_Single', 'Marital_Status_Together',
       'Marital_Status_Widow', 'Marital_Status_YOLO'])

nvo_orden=['Year_Birth', 'Education', 'Income', 'Kidhome', 'Teenhome',
       'Dt_Customer', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',
       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',
       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',
       'Complain',  'Marital_Status_Absurd', 'Marital_Status_Alone',
       'Marital_Status_Divorced', 'Marital_Status_Married',
       'Marital_Status_Single', 'Marital_Status_Together',
       'Marital_Status_Widow', 'Marital_Status_YOLO','Response']

marketing_norm=marketing_norm[nvo_orden]

marketing_norm.isnull().sum()

### realizo analisis de correlacion de variables
matriz_corr=marketing_norm.corr()

plt.figure(figsize=(20,20) )
sns.heatmap(matriz_corr, annot=True, cmap='coolwarm', linewidths=0.6,fmt=".2f")
plt.title('Mapa de Calor de Correlación')
plt.show()

"""##Modelado

Para probar la mejora que genera en la prediccion de los datos balancear las clases de clasificacion vamos a tomar una baseline el modelo de XGBoost, utilizando Precision, Recall y F1-Score
"""

### analizo si el dataset esta desbalanceado
marketing['Response'].value_counts()

"""Divido datos para entrenar modelos"""

X=marketing_norm.drop(['Response'], axis=1)
y=marketing_norm['Response']

X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=.2, random_state=36)

from sklearn.metrics import precision_score,recall_score, accuracy_score,f1_score,classification_report, confusion_matrix
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score,cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn import model_selection
from sklearn import metrics
import sklearn as sk

"""###XGBoost"""

import xgboost as xgb

# Convertimos los datos a una matriz para XGBoost
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Definimos el modelo GBM
modelo4 = xgb.XGBClassifier()

# Definir los parámetros a  probar
parametros4 = {'max_depth': [3, 5, 7],
               'learning_rate': [.01,.1,.25,.5,1],
               'n_estimators': [50,100, 200, 300],
               'subsample': [0.8, 1.0],
               'colsample_bytree': [0.8, 1.0],
               'gamma': [0, 0.1, 0.2]}

# Realizar Grid Search con validación cruzada
gridsearch4 = GridSearchCV(modelo4, parametros4, cv=5,n_jobs=-1)

# Entrenamos el modelo con la búsqueda de cuadrícula
gridsearch4.fit(X_train, y_train)

# Imprimimos los mejores parámetros y la mejor puntuación
print("Mejores parámetros encontrados:", gridsearch4.best_params_)
print("Mejor puntuación de validación cruzada:", gridsearch4.best_score_)

# Obtener las predicciones
y_pred4 = gridsearch4.predict(X_test)

print("accuracy:", sk.metrics.accuracy_score(y_test, y_pred4))
print("precision:", sk.metrics.precision_score(y_test, y_pred4))
print("recall:", sk.metrics.recall_score(y_test, y_pred4))

"""##Balanceo de clases"""

import warnings
warnings.filterwarnings("ignore")

from collections import Counter

import imblearn as im
from imblearn import under_sampling
from imblearn import over_sampling

"""###SMOTE"""

### balanceo con SMOTE y 4 vecinos
X = marketing_norm[marketing_norm.columns.drop("Response")]
y = marketing_norm["Response"]

smote = im.over_sampling.SMOTE(k_neighbors=4)
X, y = smote.fit_resample(X, y)

print(Counter(y))

###divido conjunto de datos
X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=.2, random_state=36)

y_train.value_counts()

"""APLICO MODELO CON DATOS BALANCEADOS CON SMOTE"""

# Convertimos los datos a una matriz para XGBoost
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Definimos el modelo GBM
modelo = xgb.XGBClassifier()

# Definir los parámetros a  probar
parametros  = {'max_depth': [3, 5, 7],
               'learning_rate': [.01,.1,.25,.5,1],
               'n_estimators': [50,100, 200, 300],
               'subsample': [0.8, 1.0],
               'colsample_bytree': [0.8, 1.0],
               'gamma': [0, 0.1, 0.2]}

# Realizar Grid Search con validación cruzada
gridsearch = GridSearchCV(modelo, parametros, cv=5,n_jobs=-1)

# Entrenamos el modelo con la búsqueda de cuadrícula
gridsearch.fit(X_train, y_train)

# Imprimimos los mejores parámetros y la mejor puntuación
print("Mejores parámetros encontrados:", gridsearch.best_params_)
print("Mejor puntuación de validación cruzada:", gridsearch.best_score_)

# Obtener las predicciones
y_pred = gridsearch.predict(X_test)

print("accuracy:", sk.metrics.accuracy_score(y_test, y_pred))
print("precision:", sk.metrics.precision_score(y_test, y_pred))
print("recall:", sk.metrics.recall_score(y_test, y_pred))

"""Mejora considerablemente los indicadores:
accuracy: 0.90 a 0.94
precision: 0.65 a 0.94
recall: 0.55 a 0.94

###REDES GENERATIVAS
"""

import tensorflow as tf
from tensorflow import keras

import pandas as pd
import numpy as np
import time
import IPython

from tqdm import tqdm
from tqdm.auto import trange

"""FILTRO SOLAMENTE DATOS DEL CONJUNTO MINORITARIO"""

marketing_norm_filt=marketing_norm['Response']==1
marketing_norm_filt=marketing_norm[marketing_norm_filt]
marketing_norm_filt.head()

"""Generador"""

DIM_RUIDO = 10 # hiperparámetro que hay que optimizar

modelo_generador = keras.models.Sequential([
	keras.layers.InputLayer(input_shape=[DIM_RUIDO]),

	keras.layers.Dense(33, activation="sigmoid"),
  keras.layers.Dense(11, activation="sigmoid"),

	# Capa de salida (cantidad de columnas de df)
	keras.layers.Dense(marketing_norm_filt.shape[1], activation="sigmoid")
 ])

modelo_generador.summary()

# probamos el modelo_generador

# generamos ruido al azar
ruido = tf.random.normal((1, DIM_RUIDO))

# generamos un dato
fila_generada = modelo_generador(ruido, training=False)

fila_generada

"""Discriminador"""

modelo_discriminador = keras.models.Sequential([
    keras.layers.InputLayer(input_shape=[marketing_norm_filt.shape[1]]), # entra una fila de 33 columnas

    keras.layers.Dense(33, activation="sigmoid"),
    keras.layers.Dense(11, activation="sigmoid"),

    keras.layers.Dense(1, activation="sigmoid")
])

modelo_discriminador.compile(loss="binary_crossentropy", optimizer="adam")
modelo_discriminador.summary()

# vemos cómo discrimina la fila generada anteriormente
modelo_discriminador(fila_generada)

"""Modelo adversario"""

# vamos a entrenar al discrinador a mano
modelo_discriminador.trainable = False

modelo_gan = keras.models.Sequential([
    modelo_generador,
    modelo_discriminador
])

modelo_gan.compile(loss="binary_crossentropy", optimizer="adam")

# Entrenamos el generador y el discriminador
EPOCHS = 500
BATCH_SIZE = 2

# para cada epoca...
for epoch in trange(EPOCHS, desc="epoch"):
    # para cada batch...
    with trange(marketing_norm_filt.shape[0] // BATCH_SIZE, desc="batch", leave=False) as t_batch:
        for n_batch in t_batch:
            ## Entrenamiento del modelo discriminador

            # seleccionamos BATCH_SIZE // 2 imágenes reales
            X_real = marketing_norm_filt.values[np.random.randint(0, marketing_norm_filt.shape[0], BATCH_SIZE // 2)]
            # creamos etiquetas para las imágenes reales (todas 1)
            y_real = np.ones((BATCH_SIZE // 2, 1)) * 0.9 # 0.9 --> label smoothing

            # generamos BATCH_SIZE // 2
            X_fake = modelo_generador(np.random.randn(DIM_RUIDO * BATCH_SIZE // 2).reshape(BATCH_SIZE // 2, DIM_RUIDO))
            # creamos etiquetas para las imágenes ficticias (todas 0)
            y_fake = np.zeros((BATCH_SIZE // 2, 1)) # podríamos probar con 0.1 --> label smoothing

            # creamos un conjunto de entrenamiento para el discriminador
            X_train = np.concatenate((X_real, X_fake))
            y_train = np.concatenate((y_real, y_fake))

            # actualizamos a mano los pesos del modelo discriminador
            discriminador_loss = modelo_discriminador.train_on_batch(X_train, y_train)

            ## Entrenamiento del modelo GAN (en realidad el generador)

            # Generamos un lote de ruido
            X_gan = np.random.randn(DIM_RUIDO * BATCH_SIZE).reshape(BATCH_SIZE, DIM_RUIDO)

            # Creamos etiquetas invertidas para los ejemplos ficticios (todas 1)
            y_gan = np.ones((BATCH_SIZE, 1))

            # Actualizados el generador usando el error del discriminador (el discriminador no se entrena)
            gan_loss = modelo_gan.train_on_batch(X_gan, y_gan)

            # Información sobre las pérdidas de este lote
            t_batch.set_postfix(discriminador_loss=f"{discriminador_loss:.3f}", gan_loss=f"{gan_loss:.3f}")

# mostramos las funciones de pérdida finales
print(discriminador_loss, gan_loss)


# HACER: Mejorar los registros ficticios. Por ejemplo, outcome debe ser 0 o 1, habría que redondearlo.

# generamos registros ficticios con el generador
X_fake = modelo_generador(np.random.randn(DIM_RUIDO * 1574).reshape(1574, DIM_RUIDO))
print(X_fake)

datos_sinteticos=pd.DataFrame(X_fake, columns=['Year_Birth', 'Education', 'Income', 'Kidhome', 'Teenhome',
       'Dt_Customer', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',
       'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',
       'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2',
       'Complain', 'Marital_Status_Absurd', 'Marital_Status_Alone',
       'Marital_Status_Divorced', 'Marital_Status_Married',
       'Marital_Status_Single', 'Marital_Status_Together',
       'Marital_Status_Widow', 'Marital_Status_YOLO', 'Response'])
datos_sinteticos.head(100)

datos_sinteticos['Response']=datos_sinteticos['Response'].round()
datos_sinteticos['Response'].head(10)

datos_sinteticos = pd.DataFrame(datos_sinteticos)
datos_sinteticos.head()

datos_sinteticos.columns

marketing_norm.columns

marketing_norm_gan = pd.concat([marketing_norm, datos_sinteticos])
marketing_norm_gan

marketing_norm_gan['Response'].value_counts()

"""Entreno modelo"""

X=marketing_norm_gan.drop(['Response'],axis=1)
y=marketing_norm_gan['Response']

X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=.2, random_state=36)

# Convertimos los datos a una matriz para XGBoost
import xgboost as xgb
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Definimos el modelo GBM
modelo = xgb.XGBClassifier()

# Definir los parámetros a  probar
parametros  = {'max_depth': [3, 5, 7],
               'learning_rate': [.01,.1,.25,.5,1],
               'n_estimators': [50,100, 200, 300],
               'subsample': [0.8, 1.0],
               'colsample_bytree': [0.8, 1.0],
               'gamma': [0, 0.1, 0.2]}

# Realizar Grid Search con validación cruzada
gridsearch = GridSearchCV(modelo, parametros, cv=5,n_jobs=-1)

# Entrenamos el modelo con la búsqueda de cuadrícula
gridsearch.fit(X_train, y_train)

# Imprimimos los mejores parámetros y la mejor puntuación
print("Mejores parámetros encontrados:", gridsearch.best_params_)
print("Mejor puntuación de validación cruzada:", gridsearch.best_score_)

# Obtener las predicciones
y_pred = gridsearch.predict(X_test)

print("accuracy:", sk.metrics.accuracy_score(y_test, y_pred))
print("precision:", sk.metrics.precision_score(y_test, y_pred))
print("recall:", sk.metrics.recall_score(y_test, y_pred))

"""Con el balanceo de clases generado por las redes generativas vemos que los valores de accuracy disminuyeron con respecto a SMOTE de 0.935 a 0.926, los valores de precision mejoraron de 0.928 a 0.971 y los valores de recall empeoraron de 0.945 a 0.880, con esto concluyo que el mejor modelo de balanceo de datos es el de SMOTE, sin embargo las redes generativas obtuvieron buenos resultados."""